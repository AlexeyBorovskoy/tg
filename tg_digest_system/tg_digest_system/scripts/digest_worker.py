#!/usr/bin/env python3
"""
digest_worker.py ‚Äî –ì–ª–∞–≤–Ω—ã–π –≤–æ—Ä–∫–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤.
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞: --step=text|media|ocr|digest|all
"""

import asyncio
import logging
import os
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional
import pytz

from config import Config, Channel, load_config, get_enabled_channels
from config_db import merge_channels_from_sources
from delivery_settings import (
    load_delivery_settings,
    get_delivery_settings_for_channel,
    ChannelDeliverySettings,
)
import os
from database import Database
from telegram_client import TelegramService, TelegramBot
from ocr import OCRService  # –°—Ç–∞—Ä—ã–π (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
from ocr_service_unified import UnifiedOCRService  # –ù–æ–≤—ã–π (—Å –æ–±–ª–∞—á–Ω—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏)
from llm import LLMService
from rag import vec_schema_exists, index_digest_to_rag, index_consolidated_doc_to_rag
from gitlab_push import push_to_gitlab

logger = logging.getLogger(__name__)


def _log_ctx(channel: Optional[Channel] = None, step: str = "", msg_id: Optional[int] = None, **kw) -> dict:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ª–æ–≥–æ–≤: channel, step, msg_id."""
    d = {}
    if channel is not None:
        d["channel_id"] = channel.id
        d["channel_name"] = channel.name
    if step:
        d["step"] = step
    if msg_id is not None:
        d["msg_id"] = msg_id
    d.update(kw)
    return d


class DigestWorker:
    """–í–æ—Ä–∫–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–æ–≤"""

    def __init__(self, config: Config):
        self.config = config
        self.db = Database(config)
        self.tg_service = TelegramService(config, self.db)
        self.tg_bot = TelegramBot(config)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º UnifiedOCRService –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ —Å—Ç–∞—Ä—ã–π OCRService
        if config.defaults.ocr_enabled:
            try:
                # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å
                ocr_provider = os.environ.get("OCR_PROVIDER", "tesseract").lower()
                # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –æ–±–ª–∞—á–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã: ocr_space, easyocr, google_vision, yandex_vision
                cloud_providers = ("ocr_space", "easyocr", "google_vision", "yandex_vision")
                if ocr_provider in cloud_providers or hasattr(config.defaults, 'ocr_provider'):
                    self.ocr_service = UnifiedOCRService(config, self.db)
                    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è UnifiedOCRService (–ø—Ä–æ–≤–∞–π–¥–µ—Ä: {ocr_provider})")
                else:
                    # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π Tesseract
                    self.ocr_service = OCRService(config, self.db)
                    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OCRService (Tesseract)")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å UnifiedOCRService: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º Tesseract")
                self.ocr_service = OCRService(config, self.db)
        else:
            self.ocr_service = None
        self.llm_service = LLMService(config)

    async def _get_notify_chat_id(self):
        """–ü–æ–ª—É—á–∏—Ç—å chat_id –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π (TG_STEP_NOTIFY_CHAT_ID –∏–ª–∏ user id –∏–∑ Telethon)."""
        chat_id = getattr(self.config, "tg_step_notify_chat_id", None)
        if not chat_id:
            try:
                chat_id = await self.tg_service.get_me_user_id()
            except Exception as e:
                logger.debug("Step notify: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å chat_id –∏–∑ Telethon: %s", e)
        return chat_id

    async def _notify_error_global(self, message: str) -> None:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –≤–æ—Ä–∫–µ—Ä–∞ (–±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –∫–∞–Ω–∞–ª—É). –¢–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏."""
        chat_id = await self._get_notify_chat_id()
        if not chat_id:
            return
        text = f"[TG Digest] –í–æ—Ä–∫–µ—Ä –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ: {message}"
        try:
            await self.tg_bot.send_text(chat_id, text, parse_mode="")
        except Exception as e:
            logger.warning("Notify error send failed: %s", e)

    async def _notify_step(
        self,
        channel: Channel,
        step_name: str,
        success: bool,
        message: str,
        no_messages: bool = False,
        **extra: str,
    ) -> None:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ —à–∞–≥–µ –≤ Telegram.
        - –ï—Å–ª–∏ no_messages=True –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ ¬´–ù–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç¬ª ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —á—ë—Ç–∫–æ–µ: ¬´–ù–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç. –ö–∞–Ω–∞–ª: ‚Ä¶¬ª
        - –ï—Å–ª–∏ success=False ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Ç–æ—á–Ω–∞—è –æ—à–∏–±–∫–∞: ¬´–í–æ—Ä–∫–µ—Ä –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ: ‚Ä¶¬ª
        """
        chat_id = await self._get_notify_chat_id()
        if not chat_id:
            return
        if no_messages or (success and message.strip() == "–ù–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç."):
            text = f"[TG Digest] –ù–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç. –ö–∞–Ω–∞–ª: {channel.name}"
        elif not success:
            text = f"[TG Digest] –í–æ—Ä–∫–µ—Ä –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ö–∞–Ω–∞–ª: {channel.name}, —à–∞–≥ {step_name}. –ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ: {message}"
        else:
            text = f"[TG Digest] –ö–∞–Ω–∞–ª {channel.name}, —à–∞–≥ {step_name}: {message}"
            for k, v in extra.items():
                if v:
                    text += f" {k}={v}"
        try:
            await self.tg_bot.send_text(chat_id, text, parse_mode="")
        except Exception as e:
            logger.warning("Step notify send failed: %s", e, extra=_log_ctx(channel=channel, step=step_name))

    async def process_channel(self, channel: Channel) -> Optional[int]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–Ω–∞–ª —Å —É—á—ë—Ç–æ–º user_id (–º—É–ª—å—Ç–∏—Ç–µ–Ω–∞–Ω—Ç–Ω–æ—Å—Ç—å)"""
        # –ü–æ–ª—É—á–∞–µ–º user_id –∏–∑ –∫–∞–Ω–∞–ª–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        user_id = getattr(channel, 'user_id', None)
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∫–∞–Ω–∞–ª: —Å–±–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π, OCR, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞, —Ä–∞—Å—Å—ã–ª–∫–∞.
        
        Returns:
            ID –¥–∞–π–¥–∂–µ—Å—Ç–∞ –∏–ª–∏ None
        """
        logger.info(f"=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞: {channel.name} (ID: {channel.id}) ===")
        
        # 1. –ü–æ–ª—É—á–∞–µ–º –∫—É—Ä—Å–æ—Ä (—Å —É—á—ë—Ç–æ–º user_id)
        user_id = getattr(channel, 'user_id', None)
        last_msg_id = self.db.get_last_msg_id(channel.peer_type, channel.id, user_id)
        logger.info(f"–ü–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π msg_id: {last_msg_id} (user_id={user_id})")
        
        # 2. –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        new_messages = 0
        max_msg_id = last_msg_id
        
        try:
            async for message in self.tg_service.fetch_new_messages(channel, last_msg_id):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å user_id
                await self.tg_service.save_message(message, channel, user_id=user_id)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ–¥–∏–∞ –¥–ª—è –í–°–ï–• —Å–æ–æ–±—â–µ–Ω–∏–π —Å –º–µ–¥–∏–∞ (–¥–∞–∂–µ –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —É–∂–µ –µ—Å—Ç—å –≤ –ë–î)
                if message.media and self.config.defaults.ocr_enabled:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –º–µ–¥–∏–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º user_id
                    has_media = False
                    if user_id is not None:
                        with self.db.cursor() as cur:
                            cur.execute("""
                                SELECT 1 FROM tg.media 
                                WHERE peer_type = %s AND peer_id = %s AND msg_id = %s AND user_id = %s
                                LIMIT 1
                            """, (channel.peer_type, channel.id, message.id, user_id))
                            has_media = cur.fetchone() is not None
                    else:
                        has_media = self.db.has_media_for_message(channel.peer_type, channel.id, message.id)
                    
                    if not has_media:
                        await self.tg_service.save_media(message, channel, user_id=user_id)
                
                new_messages += 1
                max_msg_id = max(max_msg_id, message.id)
                
        except Exception as e:
            logger.error(
                "–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ —Å–æ–æ–±—â–µ–Ω–∏–π: %s",
                e,
                extra=_log_ctx(channel=channel, step="process_channel"),
            )
            logger.exception("process_channel fetch traceback")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ (–µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç)
        should_create_consolidated_doc = False
        if channel.consolidated_doc_path:
            doc_path = self.config.repo_dir / channel.consolidated_doc_path
            if not doc_path.exists():
                should_create_consolidated_doc = True
                logger.info(f"–°–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –º–µ–¥–∏–∞")
        
        if new_messages == 0 and not should_create_consolidated_doc:
            logger.info("–ù–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç")
            return None
        
        if new_messages > 0:
            logger.info(f"–°–æ–±—Ä–∞–Ω–æ {new_messages} –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (–¥–æ msg_id={max_msg_id})")
        
        # 3. OCR –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ OCR (—Å —É—á–µ—Ç–æ–º user_id)
        if self.ocr_service:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ª–∏ —ç—Ç–æ —Å–µ—Ä–≤–∏—Å
            if hasattr(self.ocr_service, 'process_pending_media_async'):
                ocr_count = await self.ocr_service.process_pending_media_async(limit=50, user_id=user_id)
            else:
                # –°—Ç–∞—Ä—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
                ocr_count = self.ocr_service.process_pending_media(limit=50, user_id=user_id)
            logger.info(f"OCR –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {ocr_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º RAW –¥–∞–π–¥–∂–µ—Å—Ç
        messages = self.db.get_messages_range(
            channel.peer_type, channel.id, last_msg_id, max_msg_id
        )
        raw_digest = self._format_raw_digest(channel, messages, last_msg_id, max_msg_id)
        
        # 5. –ü–æ–ª—É—á–∞–µ–º OCR-—Ç–µ–∫—Å—Ç—ã
        ocr_texts = self.db.get_ocr_text_for_range(
            channel.peer_type, channel.id, last_msg_id, max_msg_id
        )
        
        # 6. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º LLM –¥–∞–π–¥–∂–µ—Å—Ç
        try:
            llm_digest, tokens_in, tokens_out = self.llm_service.generate_digest(
                channel, raw_digest, ocr_texts
            )
        except Exception as e:
            logger.error(
                "–û—à–∏–±–∫–∞ LLM: %s",
                e,
                extra=_log_ctx(channel=channel, step="digest"),
            )
            logger.exception("LLM digest traceback")
            llm_digest = None
            tokens_in = tokens_out = 0
        
        # 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–π–¥–∂–µ—Å—Ç –≤ –ë–î
        user_id = getattr(channel, 'user_id', None)
        digest_id = self.db.save_digest(
            peer_type=channel.peer_type,
            peer_id=channel.id,
            msg_id_from=last_msg_id,
            msg_id_to=max_msg_id,
            digest_raw=raw_digest,
            digest_llm=llm_digest,
            llm_model=self.config.openai_model if llm_digest else None,
            llm_tokens_in=tokens_in,
            llm_tokens_out=tokens_out,
            user_id=user_id,
        )
        
        # 8. –û–±–Ω–æ–≤–ª—è–µ–º –∫—É—Ä—Å–æ—Ä
        self.db.update_last_msg_id(channel.peer_type, channel.id, max_msg_id, user_id=user_id)

        # 8b. –°–≤–æ–¥–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: —Å–æ–∑–¥–∞–µ—Ç—Å—è/–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –º–µ–¥–∏–∞
        # –°–æ–∑–¥–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ (–µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç) –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        changes_summary = ""
        doc_path = self.config.repo_dir / channel.consolidated_doc_path if channel.consolidated_doc_path else None
        should_update_doc = channel.consolidated_doc_path and (new_messages > 0 or (doc_path and not doc_path.exists()))
        if should_update_doc:
            try:
                # –û–±–Ω–æ–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
                changes_summary = await self._update_consolidated_doc(channel)
                logger.info(f"–°–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç {channel.name} –æ–±–Ω–æ–≤–ª–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ {new_messages} –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
            except Exception as e:
                logger.error(
                    "–°–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç %s: %s",
                    channel.name,
                    e,
                    extra=_log_ctx(channel=channel, step="consolidated_doc"),
                )
                logger.exception("consolidated_doc traceback")

        # 9. –†–∞—Å—Å—ã–ª–∫–∞ –ø–æ–ª—É—á–∞—Ç–µ–ª—è–º –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–Ω –¥–∞–π–¥–∂–µ—Å—Ç
        # –ù–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è, –µ—Å–ª–∏ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç
        if llm_digest and new_messages > 0:
            await self._deliver_digest(
                channel, digest_id, llm_digest, last_msg_id, max_msg_id,
                changes_summary=changes_summary,
            )

        # 9b. –î–∞–π–¥–∂–µ—Å—Ç –≤ —Ñ–∞–π–ª –¥–ª—è GitLab (–¥–∞–π–¥–∂–µ—Å—Ç—ã —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –ë–î –∏ –≤ —Ä–µ–ø–æ)
        if llm_digest and self.config.gitlab_enabled:
            day_utc = datetime.now(timezone.utc).strftime("%Y-%m-%d")
            digest_dir = self.config.repo_dir / "docs" / "digests" / day_utc
            digest_dir.mkdir(parents=True, exist_ok=True)
            digest_filename = f"digest_llm_{channel.peer_type}_{channel.id}_from_{last_msg_id}_to_{max_msg_id}.md"
            digest_path = digest_dir / digest_filename
            full_digest = f"""# –î–∞–π–¥–∂–µ—Å—Ç: {channel.name}
–ü–µ—Ä–∏–æ–¥: msg_id ({last_msg_id}, {max_msg_id}]
–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")}

{llm_digest}
"""
            digest_path.write_text(full_digest, encoding="utf-8")
            self._files_to_push.append(str(digest_path.relative_to(self.config.repo_dir)))
            logger.info("–î–∞–π–¥–∂–µ—Å—Ç –∑–∞–ø–∏—Å–∞–Ω –≤ —Ñ–∞–π–ª –¥–ª—è GitLab: %s", digest_path)

        # 10. RAG: –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–∞–π–¥–∂–µ—Å—Ç (–µ—Å–ª–∏ —Å—Ö–µ–º–∞ vec –µ—Å—Ç—å)
        if llm_digest and vec_schema_exists(self.db):
            try:
                index_digest_to_rag(
                    self.config, self.db,
                    channel.peer_type, channel.id, digest_id, llm_digest, user_id=user_id
                )
            except Exception as e:
                logger.warning(f"RAG index digest: {e}")
        
        logger.info(f"=== –ö–∞–Ω–∞–ª {channel.name} –æ–±—Ä–∞–±–æ—Ç–∞–Ω, digest_id={digest_id} ===")
        return digest_id
    
    async def process_channel_daily_summary(self, channel: Channel) -> Optional[int]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –∑–∞ –¥–µ–Ω—å (–¥–∞–∂–µ –µ—Å–ª–∏ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ –±—ã–ª–æ).
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ 21:00 –ú–°–ö.
        """
        logger.info(f"=== –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç: {channel.name} (ID: {channel.id}) ===")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –¥–ª—è —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–≥–æ –¥–Ω—è (–ú–°–ö)
        date_start_utc, date_end_utc = self._get_daily_date_range()
        msk_tz = pytz.timezone("Europe/Moscow")
        date_start_msk = date_start_utc.astimezone(msk_tz)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∑–∞ –¥–µ–Ω—å
        messages = self.db.get_messages_by_date(
            channel.peer_type, channel.id, date_start_utc, date_end_utc
        )
        
        # –ü–æ–ª—É—á–∞–µ–º OCR-—Ç–µ–∫—Å—Ç—ã –∑–∞ –¥–µ–Ω—å
        ocr_texts = self.db.get_ocr_text_by_date(
            channel.peer_type, channel.id, date_start_utc, date_end_utc
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω msg_id –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ –¥–µ–Ω—å
        msg_id_from = 0
        msg_id_to = 0
        if messages:
            msg_ids = [msg["msg_id"] for msg in messages]
            msg_id_from = min(msg_ids)
            msg_id_to = max(msg_ids)
        else:
            # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π –∫—É—Ä—Å–æ—Ä
            msg_id_from = self.db.get_last_msg_id(channel.peer_type, channel.id)
            msg_id_to = msg_id_from
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º RAW –¥–∞–π–¥–∂–µ—Å—Ç
        raw_digest = self._format_daily_raw_digest(
            channel, messages, date_start_utc, date_end_utc
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º LLM –¥–∞–π–¥–∂–µ—Å—Ç
        try:
            llm_digest, tokens_in, tokens_out = self.llm_service.generate_digest(
                channel, raw_digest, ocr_texts
            )
        except Exception as e:
            logger.error(
                "–û—à–∏–±–∫–∞ LLM –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞: %s",
                e,
                extra=_log_ctx(channel=channel, step="daily_summary"),
            )
            logger.exception("LLM daily summary traceback")
            llm_digest = None
            tokens_in = tokens_out = 0
        
        if not llm_digest:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –¥–ª—è %s", channel.name)
            return None
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–π–¥–∂–µ—Å—Ç –≤ –ë–î
        user_id = getattr(channel, 'user_id', None)
        digest_id = self.db.save_digest(
            peer_type=channel.peer_type,
            peer_id=channel.id,
            msg_id_from=msg_id_from,
            msg_id_to=msg_id_to,
            digest_raw=raw_digest,
            digest_llm=llm_digest,
            llm_model=self.config.openai_model,
            llm_tokens_in=tokens_in,
            llm_tokens_out=tokens_out,
            user_id=user_id,
        )
        
        # –†–∞—Å—Å—ã–ª–∞–µ–º –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ–ª—É—á–∞—Ç–µ–ª—è–º (–¥–∞–∂–µ –µ—Å–ª–∏ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ –±—ã–ª–æ)
        await self._deliver_digest(
            channel, digest_id, llm_digest, msg_id_from, msg_id_to,
            changes_summary="",
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª –¥–ª—è GitLab
        if self.config.gitlab_enabled:
            day_utc = date_start_msk.strftime("%Y-%m-%d")
            digest_dir = self.config.repo_dir / "docs" / "digests" / day_utc
            digest_dir.mkdir(parents=True, exist_ok=True)
            digest_filename = f"daily_digest_{channel.peer_type}_{channel.id}_{day_utc}.md"
            digest_path = digest_dir / digest_filename
            full_digest = f"""# –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç: {channel.name}
–î–∞—Ç–∞: {date_start_msk.strftime('%Y-%m-%d')} (–ú–°–ö)
–ü–µ—Ä–∏–æ–¥: {date_start_msk.strftime('%Y-%m-%d %H:%M:%S')} - {date_end_utc.astimezone(msk_tz).strftime('%Y-%m-%d %H:%M:%S')} –ú–°–ö
–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")}
–°–æ–æ–±—â–µ–Ω–∏–π –∑–∞ –¥–µ–Ω—å: {len(messages)}

{llm_digest}
"""
            digest_path.write_text(full_digest, encoding="utf-8")
            self._files_to_push.append(str(digest_path.relative_to(self.config.repo_dir)))
            logger.info("–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –∑–∞–ø–∏—Å–∞–Ω –≤ —Ñ–∞–π–ª –¥–ª—è GitLab: %s", digest_path)
        
        # RAG: –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–∞–π–¥–∂–µ—Å—Ç
        user_id = getattr(channel, 'user_id', None)
        if vec_schema_exists(self.db):
            try:
                index_digest_to_rag(
                    self.config, self.db,
                    channel.peer_type, channel.id, digest_id, llm_digest, user_id=user_id
                )
            except Exception as e:
                logger.warning(f"RAG index daily digest: {e}")
        
        logger.info(f"=== –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –¥–ª—è {channel.name} –æ–±—Ä–∞–±–æ—Ç–∞–Ω, digest_id={digest_id} ===")
        return digest_id
    
    def _format_raw_digest(
        self, channel: Channel, messages: list[dict], msg_from: int, msg_to: int
    ) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç RAW –¥–∞–π–¥–∂–µ—Å—Ç"""
        ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
        
        lines = [
            f"# Increment digest",
            f"",
            f"Channel: {channel.name} (ID: {channel.id})",
            f"Window: msg_id ({msg_from}, {msg_to}]",
            f"Generated: {ts}",
            f"Messages: {len(messages)}",
            f"",
        ]
        
        for msg in messages:
            dt = msg["dt"].strftime("%Y-%m-%d %H:%M:%S") if msg["dt"] else "?"
            sender = msg.get("sender_name") or "[NO_SENDER]"
            text = (msg.get("text") or "[EMPTY]")[:1500].replace("\n", " ")
            lines.append(f"- **{dt}** `msg_id={msg['msg_id']}` **{sender}**: {text}")
        
        return "\n".join(lines)
    
    def _format_daily_raw_digest(
        self, channel: Channel, messages: list[dict], date_start: datetime, date_end: datetime
    ) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç RAW –¥–∞–π–¥–∂–µ—Å—Ç –∑–∞ –¥–µ–Ω—å"""
        msk_tz = pytz.timezone("Europe/Moscow")
        date_start_msk = date_start.astimezone(msk_tz)
        date_end_msk = date_end.astimezone(msk_tz)
        ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
        
        lines = [
            f"# Daily digest",
            f"",
            f"Channel: {channel.name} (ID: {channel.id})",
            f"Period: {date_start_msk.strftime('%Y-%m-%d')} (Moscow time)",
            f"Generated: {ts}",
            f"Messages: {len(messages)}",
            f"",
        ]
        
        if len(messages) == 0:
            lines.append("**–ù–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ –¥–µ–Ω—å –Ω–µ –±—ã–ª–æ.**")
        else:
            for msg in messages:
                dt = msg["dt"].strftime("%Y-%m-%d %H:%M:%S") if msg["dt"] else "?"
                sender = msg.get("sender_name") or "[NO_SENDER]"
                text = (msg.get("text") or "[EMPTY]")[:1500].replace("\n", " ")
                lines.append(f"- **{dt}** `msg_id={msg['msg_id']}` **{sender}**: {text}")
        
        return "\n".join(lines)
    
    def _is_daily_summary_time(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Å—Ç—É–ø–∏–ª–æ –ª–∏ –≤—Ä–µ–º—è –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ —Å–≤–æ–¥–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ (20:00 –ú–°–ö)"""
        msk_tz = pytz.timezone("Europe/Moscow")
        now_msk = datetime.now(msk_tz)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–Ω–æ 21:00-21:05 –ú–°–ö
        return now_msk.hour == 20 and now_msk.minute < 5
    
    def _get_daily_date_range(self) -> tuple[datetime, datetime]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –¥–ª—è —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–≥–æ –¥–Ω—è (00:00-23:59:59 –ú–°–ö)"""
        msk_tz = pytz.timezone("Europe/Moscow")
        now_msk = datetime.now(msk_tz)
        # –ù–∞—á–∞–ª–æ –¥–Ω—è (00:00 –ú–°–ö) - –∏—Å–ø–æ–ª—å–∑—É–µ–º replace, —Å–æ—Ö—Ä–∞–Ω—è—è timezone
        date_start_msk = now_msk.replace(hour=0, minute=0, second=0, microsecond=0)
        # –ö–æ–Ω–µ—Ü –¥–Ω—è (23:59:59 –ú–°–ö)
        date_end_msk = now_msk.replace(hour=23, minute=59, second=59, microsecond=999999)
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ UTC –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ë–î (datetime —É–∂–µ –∏–º–µ–µ—Ç timezone)
        date_start_utc = date_start_msk.astimezone(timezone.utc)
        date_end_utc = date_end_msk.astimezone(timezone.utc)
        return date_start_utc, date_end_utc

    def _consolidated_update_marker_path(self, channel: Channel) -> Path:
        """–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É-–º–∞—Ä–∫–µ—Ä—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∫–∞–Ω–∞–ª—É (—Ä–∞–∑ –≤ —Å—É—Ç–∫–∏)."""
        return self.config.repo_dir / f".last_consolidated_update_channel_{channel.id}"

    def _should_update_consolidated_doc_today(self, channel: Channel) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–µ–≥–æ–¥–Ω—è (–Ω–µ —á–∞—â–µ —Ä–∞–∑–∞ –≤ —Å—É—Ç–∫–∏)."""
        marker = self._consolidated_update_marker_path(channel)
        if not marker.exists():
            return True
        try:
            last_date = marker.read_text(encoding="utf-8").strip()
            today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
            return last_date != today
        except Exception:
            return True

    def _mark_consolidated_doc_updated_today(self, channel: Channel) -> None:
        """–û—Ç–º–µ—á–∞–µ—Ç, —á—Ç–æ —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ –∫–∞–Ω–∞–ª—É –æ–±–Ω–æ–≤–ª—ë–Ω —Å–µ–≥–æ–¥–Ω—è."""
        marker = self._consolidated_update_marker_path(channel)
        try:
            marker.parent.mkdir(parents=True, exist_ok=True)
            marker.write_text(datetime.now(timezone.utc).strftime("%Y-%m-%d"), encoding="utf-8")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –º–∞—Ä–∫–µ—Ä –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")

    def _build_consolidated_doc_link(self, channel: Channel) -> str:
        """–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–≤–æ–¥–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –≤ GitLab (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏, —Ä–∞–∑ –≤ —Å—É—Ç–∫–∏)."""
        if not self.config.gitlab_enabled or not self.config.gitlab_repo_url or not channel.consolidated_doc_path:
            return ""
        # ssh://git@gitlab.ripas.ru:8611/analyzer/analysis-methodology.git -> https://gitlab.ripas.ru/analyzer/analysis-methodology/-/blob/<branch>/<path>
        url = (
            self.config.gitlab_repo_url.strip()
            .replace("ssh://git@", "https://")
            .replace("git@", "https://")
            .replace(":8611", "")
            .rstrip("/")
        )
        if url.endswith(".git"):
            url = url[:-4]
        branch = self.config.gitlab_branch or "main"
        path = channel.consolidated_doc_path.strip("/")
        return f"{url.rstrip('/')}/-/blob/{branch}/{path}"

    async def _update_consolidated_doc(self, channel: Channel) -> str:
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –µ–¥–∏–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ —á–∞—Ç—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –º–µ–¥–∏–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—É—á–∞—Ç–µ–ª—è–º.
        """
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—ä—ë–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ API (~3‚Äì5 –º–∏–Ω): –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π –∏ OCR
        CONSOLIDATED_MSG_LIMIT = 500
        CONSOLIDATED_OCR_LIMIT = 200
        messages = self.db.get_messages_all_for_peer(channel.peer_type, channel.id, limit=CONSOLIDATED_MSG_LIMIT)
        ocr_texts = self.db.get_ocr_all_for_peer(channel.peer_type, channel.id, limit=CONSOLIDATED_OCR_LIMIT)
        # –î–∞–π–¥–∂–µ—Å—Ç—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º - –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –º–µ–¥–∏–∞
        recent_digests = []

        doc_path = self.config.repo_dir / channel.consolidated_doc_path
        previous_content = ""
        if doc_path.exists():
            try:
                previous_content = doc_path.read_text(encoding="utf-8")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: {e}")

        doc_content, changes_summary, _, _ = self.llm_service.generate_consolidated_doc(
            channel, messages, ocr_texts, recent_digests, previous_content
        )

        doc_path.parent.mkdir(parents=True, exist_ok=True)
        doc_path.write_text(doc_content, encoding="utf-8")
        logger.info(f"–°–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–ø–∏—Å–∞–Ω: {doc_path}")
        if self.config.gitlab_enabled:
            self._files_to_push.append(channel.consolidated_doc_path)

        user_id = getattr(channel, 'user_id', None)
        if vec_schema_exists(self.db):
            try:
                index_consolidated_doc_to_rag(
                    self.config, self.db,
                    channel.peer_type, channel.id,
                    channel.consolidated_doc_path, doc_content, user_id=user_id
                )
            except Exception as e:
                logger.warning(f"RAG index consolidated_doc: {e}")

        return changes_summary or ""

    async def _deliver_digest(
        self,
        channel: Channel,
        digest_id: int,
        digest_text: str,
        msg_from: int,
        msg_to: int,
        changes_summary: str = "",
    ) -> None:
        """–†–∞—Å—Å—ã–ª–∞–µ—Ç –¥–∞–π–¥–∂–µ—Å—Ç –ø–æ–ª—É—á–∞—Ç–µ–ª—è–º —Å —É—á—ë—Ç–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–æ—Å—Ç–∞–≤–∫–∏ (–ë–î –¥–ª—è –≤–µ–±-–∫–∞–Ω–∞–ª–æ–≤ –∏–ª–∏ config/digest_delivery.json)."""
        # –ö–∞–Ω–∞–ª—ã –∏–∑ –≤–µ–±–∞ (web_channels) –∏–º–µ—é—Ç –∞—Ç—Ä–∏–±—É—Ç—ã delivery_* –∏–∑ –ë–î
        if getattr(channel, "delivery_importance", None) is not None:
            delivery = ChannelDeliverySettings(
                importance=channel.delivery_importance,
                send_file=getattr(channel, "delivery_send_file", True),
                send_text=getattr(channel, "delivery_send_text", True),
                text_max_chars=getattr(channel, "delivery_text_max_chars", None),
                summary_only=getattr(channel, "delivery_summary_only", False),
            )
        else:
            delivery = get_delivery_settings_for_channel(
                channel.id,
                getattr(self, "_delivery_settings_cache", None),
            )
        ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S")
        file_name = f"digest_{channel.id}_{msg_from}_{msg_to}_{ts}.md"

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫: –¥–ª—è –∫–∞–∫–æ–≥–æ —á–∞—Ç–∞ –¥–∞–π–¥–∂–µ—Å—Ç (–¥–ª—è —Å–≤–æ–¥–Ω–æ–≥–æ –±–æ—Ç–∞)
        chat_header = (
            f"üìä *–î–∞–π–¥–∂–µ—Å—Ç –ø–æ —á–∞—Ç—É:* {channel.name}\n"
            f"–ß–∞—Ç ID: `{channel.id}`\n\n"
        )
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º –¥–æ—Å—Ç–∞–≤–∫–∏ (–æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Ç—ã)
        max_chars = delivery.text_max_chars
        if max_chars is not None and delivery.summary_only:
            short_text = (digest_text[:max_chars] + "‚Ä¶") if len(digest_text) > max_chars else digest_text
        else:
            short_text = digest_text[:3500] if len(digest_text) > 3500 else digest_text
        # –ë–ª–æ–∫ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç ‚Äî —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Ä–∞–∑ –≤ —Å—É—Ç–∫–∏)
        if changes_summary:
            doc_link = self._build_consolidated_doc_link(channel)
            short_text += (
                f"\n\n---\n_–ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å–≤–æ–¥–Ω–æ–º –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ (—á–∞—Ç: {channel.name}):_\n"
                f"{changes_summary}"
            )
            if doc_link:
                short_text += f"\n\nüìÑ [–ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç]({doc_link})"
        message_text = chat_header + short_text

        # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–∞–π–¥–∂–µ—Å—Ç–∞ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º (–¥–ª—è —Ñ–∞–π–ª–∞): —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω —á–∞—Ç
        full_digest = f"""# –î–∞–π–¥–∂–µ—Å—Ç –ø–æ —á–∞—Ç—É: {channel.name}
–ß–∞—Ç ID: {channel.id}
–ü–µ—Ä–∏–æ–¥: msg_id ({msg_from}, {msg_to}]
–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")}

{digest_text}
"""
        if changes_summary:
            full_digest += f"\n---\n## –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å–≤–æ–¥–Ω–æ–º –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ (—á–∞—Ç: {channel.name})\n\n{changes_summary}\n"
        file_data = full_digest.encode("utf-8")

        caption = f"–î–∞–π–¥–∂–µ—Å—Ç –ø–æ —á–∞—Ç—É: {channel.name} (ID: {channel.id})"
        
        user_id = getattr(channel, 'user_id', None)
        user_bot_token = getattr(channel, "user_bot_token", None) or None

        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ñ–ª–∞–≥–∏: –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —á–∞—Ç—É (digest_delivery.json) –∏ –ø–æ–ª—É—á–∞—Ç–µ–ª—å (recipient)
        do_send_text = delivery.send_text
        do_send_file = delivery.send_file

        for recipient in channel.recipients:
            if not recipient.telegram_id:
                logger.debug(f"–ü—Ä–æ–ø—É—Å–∫ –ø–æ–ª—É—á–∞—Ç–µ–ª—è {recipient.name}: telegram_id –Ω–µ –∑–∞–¥–∞–Ω")
                continue
            send_text = do_send_text and recipient.send_text
            send_file = do_send_file and recipient.send_file
            try:
                if send_text:
                    success = await self.tg_bot.send_text(
                        recipient.telegram_id,
                        message_text,
                        parse_mode="Markdown",
                        bot_token=user_bot_token,
                    )
                    self.db.save_delivery(
                        digest_id=digest_id,
                        telegram_id=recipient.telegram_id,
                        delivery_type="text",
                        status="sent" if success else "failed",
                        user_id=user_id,
                    )
                
                if send_file:
                    success = await self.tg_bot.send_document_bytes(
                        recipient.telegram_id,
                        file_data,
                        file_name,
                        caption=caption,
                        bot_token=user_bot_token,
                    )
                    self.db.save_delivery(
                        digest_id=digest_id,
                        telegram_id=recipient.telegram_id,
                        delivery_type="file",
                        status="sent" if success else "failed",
                        user_id=user_id,
                    )
                
                logger.info(
                    "–î–æ—Å—Ç–∞–≤–ª–µ–Ω–æ %s (ID: %s) [text=%s file=%s importance=%s]",
                    recipient.name, recipient.telegram_id, send_text, send_file, delivery.importance,
                )
                
            except Exception as e:
                logger.error(
                    "–û—à–∏–±–∫–∞ –¥–æ—Å—Ç–∞–≤–∫–∏ –¥–ª—è %s: %s",
                    recipient.name,
                    e,
                    extra=_log_ctx(channel=channel, msg_id=digest_id),
                )
                logger.exception("deliver traceback")

    # -------------------------------------------------------------------------
    # –ü–æ—à–∞–≥–æ–≤—ã–π —Ä–µ–∂–∏–º (--step=text|media|ocr|digest)
    # -------------------------------------------------------------------------

    async def process_channel_step_text(self, channel: Channel) -> None:
        """–®–∞–≥ 1: —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –∫—É—Ä—Å–æ—Ä, –ø–µ—Ä–≤—ã–π —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (–±–µ–∑ –º–µ–¥–∏–∞/OCR)."""
        step_name = "text"
        logger.info(
            "Step %s started",
            step_name,
            extra=_log_ctx(channel=channel, step=step_name),
        )
        try:
            last_msg_id = self.db.get_last_msg_id(channel.peer_type, channel.id)
            logger.info(
                "Step %s: last_msg_id=%s",
                step_name,
                last_msg_id,
                extra=_log_ctx(channel=channel, step=step_name),
            )
            new_messages = 0
            max_msg_id = last_msg_id
            last_message = None
            try:
                async for message in self.tg_service.fetch_new_messages(channel, last_msg_id):
                    last_message = message
                    await self.tg_service.save_message(message, channel)
                    new_messages += 1
                    max_msg_id = max(max_msg_id, message.id)
                    logger.debug(
                        "Step %s: msg_id=%s saved",
                        step_name,
                        message.id,
                        extra=_log_ctx(channel=channel, step=step_name, msg_id=message.id),
                    )
            except Exception as e:
                logger.exception(
                    "Step %s: fetch/save FAIL msg_id=%s: %s",
                    step_name,
                    getattr(last_message, "id", "?"),
                    e,
                    extra=_log_ctx(channel=channel, step=step_name),
                )
                await self._notify_step(
                    channel,
                    step_name,
                    success=False,
                    message=f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞: {e}",
                )
                return

            if new_messages == 0:
                logger.info(
                    "Step %s: –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç",
                    step_name,
                    extra=_log_ctx(channel=channel, step=step_name),
                )
                await self._notify_step(channel, step_name, success=True, message="–ù–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç.", no_messages=True)
                return

            self.db.update_last_msg_id(channel.peer_type, channel.id, max_msg_id, user_id=user_id)
            logger.info(
                "Step %s: —Å–æ–±—Ä–∞–Ω–æ %s —Å–æ–æ–±—â–µ–Ω–∏–π, –∫—É—Ä—Å–æ—Ä –æ–±–Ω–æ–≤–ª—ë–Ω –¥–æ %s",
                step_name,
                new_messages,
                max_msg_id,
                extra=_log_ctx(channel=channel, step=step_name),
            )

            # –°–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –º–µ–¥–∏–∞ (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤)
            if channel.consolidated_doc_path:
                try:
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—ä—ë–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ API (~3‚Äì5 –º–∏–Ω)
                    messages = self.db.get_messages_all_for_peer(channel.peer_type, channel.id, limit=500)
                    ocr_texts = self.db.get_ocr_all_for_peer(channel.peer_type, channel.id, limit=200)
                    # –î–∞–π–¥–∂–µ—Å—Ç—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º - –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –º–µ–¥–∏–∞
                    recent_digests = []
                    previous_content = ""
                    doc_path = self.config.repo_dir / channel.consolidated_doc_path
                    if doc_path.exists():
                        try:
                            previous_content = doc_path.read_text(encoding="utf-8")
                        except Exception as e:
                            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: %s", e)
                    doc_content, changes_summary, _, _ = self.llm_service.generate_consolidated_doc(
                        channel, messages, ocr_texts, recent_digests, previous_content
                    )
                    doc_path.parent.mkdir(parents=True, exist_ok=True)
                    doc_path.write_text(doc_content, encoding="utf-8")
                    logger.info(
                        "Step %s: —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–ø–∏—Å–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ %s —Å–æ–æ–±—â–µ–Ω–∏–π –∏ %s OCR —Ç–µ–∫—Å—Ç–æ–≤: %s",
                        step_name,
                        len(messages),
                        len(ocr_texts),
                        doc_path,
                        extra=_log_ctx(channel=channel, step=step_name),
                    )
                    if self.config.gitlab_enabled:
                        self._files_to_push.append(channel.consolidated_doc_path)
                except Exception as e:
                    logger.exception(
                        "Step %s: —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç FAIL: %s",
                        step_name,
                        e,
                        extra=_log_ctx(channel=channel, step=step_name),
                    )
                    await self._notify_step(
                        channel,
                        step_name,
                        success=False,
                        message=f"–°–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: {e}",
                    )
                    return

            await self._notify_step(
                channel,
                step_name,
                success=True,
                message=f"–°–æ–æ–±—â–µ–Ω–∏–π: {new_messages}, –∫—É—Ä—Å–æ—Ä: {max_msg_id}.",
                doc_path=channel.consolidated_doc_path or "",
            )
            logger.info(
                "Step %s finished: total=%s cursor=%s",
                step_name,
                new_messages,
                max_msg_id,
                extra=_log_ctx(channel=channel, step=step_name),
            )
        except Exception as e:
            logger.exception(
                "Step %s FAIL: %s",
                step_name,
                e,
                extra=_log_ctx(channel=channel, step=step_name),
            )
            await self._notify_step(channel, step_name, success=False, message=str(e))

    async def process_channel_step_media(self, channel: Channel) -> None:
        """–®–∞–≥ 2: –∑–∞–≥—Ä—É–∑–∫–∞ –º–µ–¥–∏–∞ –≤ –ë–î –¥–ª—è –í–°–ï–• —Å–æ–æ–±—â–µ–Ω–∏–π —Å –º–µ–¥–∏–∞ (–¥–∞–∂–µ –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏—è —É–∂–µ –µ—Å—Ç—å –≤ –ë–î)."""
        step_name = "media"
        logger.info(
            "Step %s started",
            step_name,
            extra=_log_ctx(channel=channel, step=step_name),
        )
        try:
            user_id = getattr(channel, 'user_id', None)
            total = 0
            failed = 0
            
            await self.tg_service.connect()
            entity = await self.tg_service._client.get_entity(channel.id)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï —Å–æ–æ–±—â–µ–Ω–∏—è —Å –º–µ–¥–∏–∞ (–æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º)
            try:
                async for message in self.tg_service._client.iter_messages(entity, reverse=True, limit=None):
                    if not message.media:
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –º–µ–¥–∏–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º user_id
                    has_media = False
                    if user_id is not None:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å —É—á–µ—Ç–æ–º user_id
                        with self.db.cursor() as cur:
                            cur.execute("""
                                SELECT 1 FROM tg.media 
                                WHERE peer_type = %s AND peer_id = %s AND msg_id = %s AND user_id = %s
                                LIMIT 1
                            """, (channel.peer_type, channel.id, message.id, user_id))
                            has_media = cur.fetchone() is not None
                    else:
                        has_media = self.db.has_media_for_message(channel.peer_type, channel.id, message.id)
                    
                    if has_media:
                        logger.debug(
                            "Step %s: msg_id=%s —É–∂–µ –µ—Å—Ç—å –º–µ–¥–∏–∞ (user_id=%s), –ø—Ä–æ–ø—É—Å–∫",
                            step_name, message.id, user_id,
                            extra=_log_ctx(channel=channel, step=step_name, msg_id=message.id),
                        )
                        continue
                    
                    try:
                        media_id = await self.tg_service.save_media(message, channel, user_id=user_id)
                        if media_id:
                            total += 1
                        logger.debug(
                            "Step %s: msg_id=%s media_id=%s OK",
                            step_name,
                            message.id,
                            media_id,
                            extra=_log_ctx(channel=channel, step=step_name, msg_id=message.id),
                        )
                    except Exception as e:
                        failed += 1
                        logger.warning(
                            "Step %s: msg_id=%s FAIL: %s",
                            step_name,
                            message.id,
                            e,
                            extra=_log_ctx(channel=channel, step=step_name, msg_id=message.id),
                        )
                        await self._notify_step(
                            channel,
                            step_name,
                            success=False,
                            message=f"msg_id={message.id} FAIL: {e}",
                        )
            except Exception as e:
                logger.exception(
                    "Step %s: fetch FAIL: %s",
                    step_name,
                    e,
                    extra=_log_ctx(channel=channel, step=step_name),
                )
                await self._notify_step(channel, step_name, success=False, message=f"–û—à–∏–±–∫–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏: {e}")
                return

            await self._notify_step(
                channel,
                step_name,
                success=True,
                message=f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {total}, –æ—à–∏–±–æ–∫: {failed}.",
            )
            logger.info(
                "Step %s finished: total=%s failed=%s",
                step_name,
                total,
                failed,
                extra=_log_ctx(channel=channel, step=step_name),
            )
        except Exception as e:
            logger.exception(
                "Step %s FAIL: %s",
                step_name,
                e,
                extra=_log_ctx(channel=channel, step=step_name),
            )
            await self._notify_step(channel, step_name, success=False, message=str(e))

    async def process_channel_step_ocr(self, channel: Channel) -> None:
        """–®–∞–≥ 3: OCR –ø–æ –æ–¥–Ω–æ–º—É –º–µ–¥–∏–∞."""
        step_name = "ocr"
        logger.info(
            "Step %s started",
            step_name,
            extra=_log_ctx(channel=channel, step=step_name),
        )
        if not self.ocr_service:
            logger.warning("Step %s: OCR –æ—Ç–∫–ª—é—á—ë–Ω –≤ –∫–æ–Ω—Ñ–∏–≥–µ", step_name)
            await self._notify_step(channel, step_name, success=True, message="OCR –æ—Ç–∫–ª—é—á—ë–Ω.")
            return
        try:
            user_id = getattr(channel, 'user_id', None)
            processed = 0
            failed = 0
            while True:
                media_list = self.db.get_media_without_ocr(limit=1, user_id=user_id)
                if not media_list:
                    break
                m = media_list[0]
                media_id = m["id"]
                msg_id = m["msg_id"]
                peer_type = m["peer_type"]
                peer_id = m["peer_id"]
                media_user_id = m.get("user_id") or user_id
                try:
                    file_data = m.get("file_data")
                    if file_data is not None:
                        file_data = bytes(file_data)
                    elif m.get("local_path"):
                        file_data = Path(m["local_path"]).read_bytes()
                    else:
                        logger.warning("Step %s: media_id=%s –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö", step_name, media_id)
                        failed += 1
                        continue
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ª–∏ —ç—Ç–æ –º–µ—Ç–æ–¥
                    if asyncio.iscoroutinefunction(self.ocr_service.process_image):
                        text, metadata = await self.ocr_service.process_image(file_data)
                        ocr_model = metadata.get('provider', 'unknown')
                    else:
                        # –°—Ç–∞—Ä—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
                        text, _ = self.ocr_service.process_image(file_data)
                        ocr_model = "tesseract"
                    
                    self.db.save_ocr_text(
                        media_id=media_id,
                        peer_type=peer_type,
                        peer_id=peer_id,
                        msg_id=msg_id,
                        ocr_text=text or "",
                        ocr_model=ocr_model,
                        user_id=media_user_id,
                    )
                    processed += 1
                    logger.debug(
                        "Step %s: media_id=%s msg_id=%s OK",
                        step_name,
                        media_id,
                        msg_id,
                        extra=_log_ctx(channel=channel, step=step_name, msg_id=msg_id),
                    )
                except Exception as e:
                    failed += 1
                    logger.warning(
                        "Step %s: media_id=%s msg_id=%s FAIL: %s",
                        step_name,
                        media_id,
                        msg_id,
                        e,
                        extra=_log_ctx(channel=channel, step=step_name, msg_id=msg_id),
                    )
                    await self._notify_step(
                        channel,
                        step_name,
                        success=False,
                        message=f"media_id={media_id} FAIL: {e}",
                    )

            await self._notify_step(
                channel,
                step_name,
                success=True,
                message=f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}, –æ—à–∏–±–æ–∫: {failed}.",
            )
            logger.info(
                "Step %s finished: processed=%s failed=%s",
                step_name,
                processed,
                failed,
                extra=_log_ctx(channel=channel, step=step_name),
            )
        except Exception as e:
            logger.exception(
                "Step %s FAIL: %s",
                step_name,
                e,
                extra=_log_ctx(channel=channel, step=step_name),
            )
            await self._notify_step(channel, step_name, success=False, message=str(e))

    async def process_channel_step_digest(self, channel: Channel) -> Optional[int]:
        """–®–∞–≥ 4: —Ç–æ–ª—å–∫–æ –¥–∞–π–¥–∂–µ—Å—Ç –∏ —Å–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (–¥–∞–Ω–Ω—ã–µ —É–∂–µ –≤ –ë–î)."""
        step_name = "digest"
        logger.info(
            "Step %s started",
            step_name,
            extra=_log_ctx(channel=channel, step=step_name),
        )
        try:
            last_msg_id = self.db.get_last_msg_id(channel.peer_type, channel.id)
            max_msg_id = self.db.get_max_msg_id(channel.peer_type, channel.id)
            if max_msg_id <= last_msg_id:
                logger.info(
                    "Step %s: –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç (last=%s max=%s)",
                    step_name,
                    last_msg_id,
                    max_msg_id,
                    extra=_log_ctx(channel=channel, step=step_name),
                )
                await self._notify_step(channel, step_name, success=True, message="–ù–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ—Ç.", no_messages=True)
                return None
            messages = self.db.get_messages_range(
                channel.peer_type, channel.id, last_msg_id, max_msg_id
            )
            user_id = getattr(channel, "user_id", None)
            new_messages = len(messages)
            raw_digest = self._format_raw_digest(channel, messages, last_msg_id, max_msg_id)
            ocr_texts = self.db.get_ocr_text_for_range(
                channel.peer_type, channel.id, last_msg_id, max_msg_id
            )
            try:
                llm_digest, tokens_in, tokens_out = self.llm_service.generate_digest(
                    channel, raw_digest, ocr_texts
                )
            except Exception as e:
                logger.exception("Step %s: LLM FAIL: %s", step_name, e, extra=_log_ctx(channel=channel, step=step_name))
                await self._notify_step(channel, step_name, success=False, message=f"LLM: {e}")
                return None
            digest_id = self.db.save_digest(
                peer_type=channel.peer_type,
                peer_id=channel.id,
                msg_id_from=last_msg_id,
                msg_id_to=max_msg_id,
                digest_raw=raw_digest,
                digest_llm=llm_digest,
                llm_model=self.config.openai_model,
                llm_tokens_in=tokens_in,
                llm_tokens_out=tokens_out,
            )
            self.db.update_last_msg_id(channel.peer_type, channel.id, max_msg_id, user_id=user_id)
            changes_summary = ""
            # –°–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–µ—Ç—Å—è/–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –º–µ–¥–∏–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if channel.consolidated_doc_path and new_messages > 0:
                try:
                    changes_summary = await self._update_consolidated_doc(channel)
                    logger.info(f"–°–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç {channel.name} –æ–±–Ω–æ–≤–ª–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ {new_messages} –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
                except Exception as e:
                    logger.exception("Step %s: consolidated_doc FAIL: %s", step_name, e)
            if llm_digest:
                await self._deliver_digest(
                    channel, digest_id, llm_digest, last_msg_id, max_msg_id,
                    changes_summary=changes_summary,
                )
            if llm_digest and self.config.gitlab_enabled:
                day_utc = datetime.now(timezone.utc).strftime("%Y-%m-%d")
                digest_dir = self.config.repo_dir / "docs" / "digests" / day_utc
                digest_dir.mkdir(parents=True, exist_ok=True)
                digest_filename = f"digest_llm_{channel.peer_type}_{channel.id}_from_{last_msg_id}_to_{max_msg_id}.md"
                digest_path = digest_dir / digest_filename
                full_digest = f"""# –î–∞–π–¥–∂–µ—Å—Ç: {channel.name}\n–ü–µ—Ä–∏–æ–¥: msg_id ({last_msg_id}, {max_msg_id}]\n\n{llm_digest}\n"""
                digest_path.write_text(full_digest, encoding="utf-8")
                self._files_to_push.append(str(digest_path.relative_to(self.config.repo_dir)))
            await self._notify_step(
                channel,
                step_name,
                success=True,
                message=f"digest_id={digest_id} msg_id={last_msg_id}-{max_msg_id}.",
            )
            logger.info(
                "Step %s finished: digest_id=%s",
                step_name,
                digest_id,
                extra=_log_ctx(channel=channel, step=step_name),
            )
            return digest_id
        except Exception as e:
            logger.exception(
                "Step %s FAIL: %s",
                step_name,
                e,
                extra=_log_ctx(channel=channel, step=step_name),
            )
            await self._notify_step(channel, step_name, success=False, message=str(e))
            return None

    async def run_once(self, step: Optional[str] = None) -> None:
        """–û–¥–∏–Ω —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–æ–≤. step: text|media|ocr|digest|all (None = all)."""
        self._files_to_push = []
        # –ö—ç—à –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–æ—Å—Ç–∞–≤–∫–∏ –Ω–∞ —Ü–∏–∫–ª (config/digest_delivery.json)
        self._delivery_settings_cache = load_delivery_settings()
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–Ω–∞–ª—ã –∏–∑ –ë–î –∏ —Ñ–∞–π–ª–∞ (–º—É–ª—å—Ç–∏—Ç–µ–Ω–∞–Ω—Ç–Ω–æ—Å—Ç—å)
        merged_channels = merge_channels_from_sources(self.config)
        self.config.channels = merged_channels
        channels = get_enabled_channels(self.config)
        step_mode = (step or "all").lower()
        logger.info(
            "–ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ %s –∫–∞–Ω–∞–ª–æ–≤, step=%s",
            len(channels),
            step_mode,
            extra={"step": step_mode},
        )

        for channel in channels:
            try:
                if step_mode == "text":
                    await self.process_channel_step_text(channel)
                elif step_mode == "media":
                    await self.process_channel_step_media(channel)
                elif step_mode == "ocr":
                    await self.process_channel_step_ocr(channel)
                elif step_mode == "digest":
                    await self.process_channel_step_digest(channel)
                else:
                    await self.process_channel(channel)
            except Exception as e:
                logger.error(
                    "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ %s: %s",
                    channel.name,
                    e,
                    extra=_log_ctx(channel=channel, step=step_mode),
                )
                logger.exception("run_once traceback")

            await asyncio.sleep(2)

        # –°–≤–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ –≤–Ω—É—Ç—Ä–∏ process_channel (—Å–º. —à–∞–≥ 8b)

        # –ü—É—à –≤ GitLab (gitlab.ripas.ru): –¥–∞–π–¥–∂–µ—Å—Ç—ã –∏ —Å–≤–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if self._files_to_push and self.config.gitlab_enabled:
            try:
                msg = "digests and docs " + datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
                ok = push_to_gitlab(
                    self.config.repo_dir,
                    self._files_to_push,
                    msg,
                    branch=self.config.gitlab_branch,
                    ssh_key_path=self.config.gitlab_ssh_key or "",
                )
                if not ok:
                    logger.warning("GitLab push –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω")
            except Exception as e:
                logger.error("GitLab push: %s", e)
        
        await self.tg_service.disconnect()

        # Heartbeat –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: –ø–æ—Å–ª–µ–¥–Ω–∏–π —É—Å–ø–µ—à–Ω—ã–π —Ü–∏–∫–ª (healthcheck –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç —Ñ–∞–π–ª–∞)
        try:
            heartbeat_dir = Path(self.config.logs_dir)
            heartbeat_dir.mkdir(parents=True, exist_ok=True)
            (heartbeat_dir / "heartbeat.txt").write_text(
                datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
                encoding="utf-8",
            )
        except Exception as e:
            logger.warning(f"Heartbeat –Ω–µ –∑–∞–ø–∏—Å–∞–Ω: {e}")
    
    async def run_loop(self, interval_minutes: int = 30) -> None:
        """–ó–∞–ø—É—Å–∫ –≤ —Ä–µ–∂–∏–º–µ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞"""
        logger.info(f"–ó–∞–ø—É—Å–∫ –≤–æ—Ä–∫–µ—Ä–∞ –≤ —Ä–µ–∂–∏–º–µ —Ü–∏–∫–ª–∞ (–∏–Ω—Ç–µ—Ä–≤–∞–ª: {interval_minutes} –º–∏–Ω)")
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å, –∫–æ–≥–¥–∞ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç
        last_daily_summary_date = None
        config_file = Path(self.config.repo_dir) / "config" / "channels.json"
        if not config_file.exists():
            config_file = Path(os.environ.get("CONFIG_FILE", str(config_file)))
        
        while True:
            try:
                # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Ü–∏–∫–ª–æ–º (–¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫–∞–Ω–∞–ª–æ–≤)
                try:
                    new_config = load_config(str(config_file))
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ –º–µ–Ω—è–µ–º
                    old_channel_ids = {ch.id for ch in self.config.channels}
                    new_channel_ids = {ch.id for ch in new_config.channels}
                    
                    if old_channel_ids != new_channel_ids:
                        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∫–∞–Ω–∞–ª–æ–≤. –ë—ã–ª–æ: {len(old_channel_ids)}, —Å—Ç–∞–ª–æ: {len(new_channel_ids)}")
                        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –≤–æ—Ä–∫–µ—Ä —Å –Ω–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
                        self.config = new_config
                        self.db = Database(self.config)
                        self.tg_service = TelegramService(self.config, self.db)
                        # –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OCR –∏ LLM —Å–µ—Ä–≤–∏—Å—ã
                        if self.config.defaults.ocr_enabled:
                            try:
                                ocr_provider = os.environ.get("OCR_PROVIDER", "tesseract").lower()
                                cloud_providers = ("ocr_space", "easyocr", "google_vision", "yandex_vision")
                                if ocr_provider in cloud_providers or hasattr(self.config.defaults, 'ocr_provider'):
                                    self.ocr_service = UnifiedOCRService(self.config, self.db)
                                else:
                                    self.ocr_service = OCRService(self.config, self.db)
                            except Exception as e:
                                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å OCR: {e}")
                                self.ocr_service = OCRService(self.config, self.db)
                        else:
                            self.ocr_service = None
                        self.llm_service = LLMService(self.config)
                        logger.info("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–æ–≤—ã–µ –∫–∞–Ω–∞–ª—ã –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Å—Ç—É–ø–∏–ª–æ –ª–∏ –≤—Ä–µ–º—è –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ —Å–≤–æ–¥–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ (20:00 –ú–°–ö)
                msk_tz = pytz.timezone("Europe/Moscow")
                now_msk = datetime.now(msk_tz)
                today_date = now_msk.date()
                
                if self._is_daily_summary_time() and last_daily_summary_date != today_date:
                    logger.info("–í—Ä–µ–º—è –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ —Å–≤–æ–¥–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ (20:00 –ú–°–ö)")
                    channels = get_enabled_channels(self.config)
                    for channel in channels:
                        try:
                            await self.process_channel_daily_summary(channel)
                        except Exception as e:
                            logger.error(
                                "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ –¥–ª—è %s: %s",
                                channel.name,
                                e,
                                extra=_log_ctx(channel=channel, step="daily_summary"),
                            )
                            logger.exception("daily_summary traceback")
                        await asyncio.sleep(2)
                    
                    last_daily_summary_date = today_date
                    logger.info("–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–µ —Å–≤–æ–¥–Ω—ã–µ –¥–∞–π–¥–∂–µ—Å—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
                
                # –û–±—ã—á–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–Ω–∞–ª–æ–≤
                await self.run_once()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ: {e}")
                try:
                    await self._notify_error_global(str(e))
                except Exception as notify_err:
                    logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ: %s", notify_err)
            
            logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ {interval_minutes} –º–∏–Ω—É—Ç...")
            await asyncio.sleep(interval_minutes * 60)


async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    import argparse
    
    parser = argparse.ArgumentParser(description="TG Digest Worker")
    parser.add_argument("--config", default=None, help="–ü—É—Ç—å –∫ channels.json")
    parser.add_argument("--once", action="store_true", help="–û–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã–π –∑–∞–ø—É—Å–∫")
    parser.add_argument("--interval", type=int, default=30, help="–ò–Ω—Ç–µ—Ä–≤–∞–ª –≤ –º–∏–Ω—É—Ç–∞—Ö")
    parser.add_argument("--channel", type=int, help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–π –∫–∞–Ω–∞–ª")
    parser.add_argument(
        "--step",
        choices=("text", "media", "ocr", "digest", "all"),
        default="all",
        help="–®–∞–≥ –ø–∞–π–ø–ª–∞–π–Ω–∞: text (—Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è+–¥–æ–∫—É–º–µ–Ω—Ç), media (–∑–∞–≥—Ä—É–∑–∫–∞ –º–µ–¥–∏–∞), ocr, digest, all (–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª)",
    )
    parser.add_argument("--debug", action="store_true", help="Debug —Ä–µ–∂–∏–º")
    args = parser.parse_args()
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    log_level = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
    config = load_config(args.config)
    
    # –°–æ–∑–¥–∞—ë–º –≤–æ—Ä–∫–µ—Ä
    worker = DigestWorker(config)
    
    if args.channel:
        channel = next((c for c in config.channels if c.id == args.channel), None)
        if channel:
            step = args.step if args.step != "all" else None
            if step == "text":
                await worker.process_channel_step_text(channel)
            elif step == "media":
                await worker.process_channel_step_media(channel)
            elif step == "ocr":
                await worker.process_channel_step_ocr(channel)
            elif step == "digest":
                await worker.process_channel_step_digest(channel)
            else:
                await worker.process_channel(channel)
        else:
            logger.error("–ö–∞–Ω–∞–ª %s –Ω–µ –Ω–∞–π–¥–µ–Ω", args.channel)
    elif args.once:
        step = args.step if args.step != "all" else None
        await worker.run_once(step=step)
    else:
        await worker.run_loop(args.interval)


if __name__ == "__main__":
    asyncio.run(main())
